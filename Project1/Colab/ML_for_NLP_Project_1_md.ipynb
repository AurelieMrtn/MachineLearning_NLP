{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSu0uObpY1Gj"
      },
      "source": [
        "## NFCorpusBM25.ipynb :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAxnJlk-WjX9",
        "outputId": "77765e4b-f853-4587-ab59-65df153f077e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rank_bm25 in /usr/local/lib/python3.10/dist-packages (0.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rank_bm25) (1.23.5)\n",
            "fatal: destination path 'project1-2023' already exists and is not an empty directory.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ndcg bm25= 0.43555347139300205\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.43555347139300205"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install rank_bm25\n",
        "!git clone https://github.com/cr-nlp/project1-2023.git\n",
        "\n",
        "import urllib.request as re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "from sklearn.metrics import ndcg_score\n",
        "from rank_bm25 import BM25Okapi\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "def loadNFCorpus():\n",
        "\tdir = \"./project1-2023/\"\n",
        "\n",
        "\tfilename = dir +\"dev.docs\"\n",
        "\tdicDoc={}\n",
        "\twith open(filename) as file:\n",
        "\t\tlines = file.readlines()\n",
        "\tfor line in lines:\n",
        "\t\ttabLine = line.split('\\t')\n",
        "\t\tkey = tabLine[0]\n",
        "\t\tvalue = tabLine[1]\n",
        "\t\tdicDoc[key] = value\n",
        "\n",
        "\tfilename = dir + \"dev.all.queries\"\n",
        "\tdicReq={}\n",
        "\twith open(filename) as file:\n",
        "\t\tlines = file.readlines()\n",
        "\tfor line in lines:\n",
        "\t\ttabLine = line.split('\\t')\n",
        "\t\tkey = tabLine[0]\n",
        "\t\tvalue = tabLine[1]\n",
        "\t\tdicReq[key] = value\n",
        "\n",
        "\tfilename = dir + \"dev.2-1-0.qrel\"\n",
        "\tdicReqDoc=defaultdict(dict)\n",
        "\twith open(filename) as file:\n",
        "\t\tlines = file.readlines()\n",
        "\tfor line in lines:\n",
        "\t\ttabLine = line.strip().split('\\t')\n",
        "\t\treq = tabLine[0]\n",
        "\t\tdoc = tabLine[2]\n",
        "\t\tscore = int(tabLine[3])\n",
        "\t\tdicReqDoc[req][doc]=score\n",
        "\n",
        "\treturn dicDoc, dicReq, dicReqDoc\n",
        "\n",
        "\n",
        "def text2TokenList(text):\n",
        "\tstopword = stopwords.words('english')\n",
        "\t#print(\"LEN DE STOPWORD=\",len(stopword))\n",
        "\tword_tokens = word_tokenize(text.lower())\n",
        "\tword_tokens_without_stops = [word for word in word_tokens if word not in stopword and len(word)>2]\n",
        "\treturn word_tokens_without_stops\n",
        "\n",
        "\n",
        "def run_bm25_only(startDoc,endDoc):\n",
        "\n",
        "\tdicDoc, dicReq, dicReqDoc = loadNFCorpus()\n",
        "\n",
        "\tdocsToKeep=[]\n",
        "\treqsToKeep=[]\n",
        "\tdicReqDocToKeep=defaultdict(dict)\n",
        "\n",
        "\tndcgTop=10\n",
        "\t#print(\"ndcgTop=\",ndcgTop,\"nbDocsToKeep=\",nbDocsToKeep)\n",
        "\n",
        "\ti=startDoc\n",
        "\tfor reqId in dicReqDoc:\n",
        "\t\tif i > (endDoc - startDoc) :  #nbDocsToKeep:\n",
        "\t\t\tbreak\n",
        "\t\tfor docId in dicReqDoc[reqId]:\n",
        "\t\t\tdicReqDocToKeep[reqId][docId] = dicReqDoc[reqId][docId]\n",
        "\t\t\tdocsToKeep.append(docId)\n",
        "\t\t\ti = i + 1\n",
        "\t\treqsToKeep.append(reqId)\n",
        "\tdocsToKeep = list(set(docsToKeep))\n",
        "\n",
        "\t# Creates list of voc for docs and reqs:\n",
        "\tallVocab ={}\n",
        "\tfor k in docsToKeep:\n",
        "\t\tdocTokenList = text2TokenList(dicDoc[k])\n",
        "\t\tfor word in docTokenList:\n",
        "\t\t\tif word not in allVocab:\n",
        "\t\t\t\tallVocab[word] = word\n",
        "\tallVocabListDoc = list(allVocab)\n",
        "\n",
        "\tallVocab ={}\n",
        "\tfor k in reqsToKeep:\n",
        "\t\tdocTokenList = text2TokenList(dicReq[k])\n",
        "\t\tfor word in docTokenList:\n",
        "\t\t\tif word not in allVocab:\n",
        "\t\t\t\tallVocab[word] = word\n",
        "\tallVocabListReq = list(allVocab)\n",
        "\n",
        "\tcorpusDocTokenList = []\n",
        "\tcorpusReqTokenList = {}\n",
        "\n",
        "# Creates token lists for docs and reqs as well as dict of docs and reqs:\n",
        "  # corpusDocName == docsToKeep and corpusReqName == reqsToKeep\n",
        "\tcorpusDocName=[]\n",
        "\tcorpusDicoDocName={}\n",
        "\ti = 0\n",
        "\tfor k in docsToKeep:\n",
        "\t\tdocTokenList = text2TokenList(dicDoc[k])\n",
        "\t\tcorpusDocTokenList.append(docTokenList)\n",
        "\t\tcorpusDocName.append(k)\n",
        "\t\tcorpusDicoDocName[k] = i\n",
        "\t\ti = i + 1\n",
        "\n",
        "\tcorpusReqName=[]\n",
        "\tcorpusDicoReqName={}\n",
        "\ti = 0\n",
        "\tfor k in reqsToKeep:\n",
        "\t\treqTokenList = text2TokenList(dicReq[k])\n",
        "\t\tcorpusReqTokenList[k] = reqTokenList\n",
        "\t\tcorpusReqName.append(k)\n",
        "\t\tcorpusDicoReqName[k] = i\n",
        "\t\ti = i + 1\n",
        "\n",
        "\tbm25 = BM25Okapi(corpusDocTokenList)\n",
        "\n",
        "\tndcgCumul=0\n",
        "\tcorpusReqVec={}\n",
        "\tndcgBM25Cumul=0\n",
        "\tnbReq=0\n",
        "\n",
        "\tfor req in corpusReqTokenList:\n",
        "\t\tj=0\n",
        "\t\treqTokenList = corpusReqTokenList[req]\n",
        "\t\tdoc_scores = bm25.get_scores(reqTokenList)\n",
        "\t\ttrueDocs = np.zeros(len(corpusDocTokenList))\n",
        "\n",
        "\t\tfor docId in corpusDicoDocName:\n",
        "\t\t\tif req in dicReqDocToKeep:\n",
        "\t\t\t\tif docId in dicReqDocToKeep[req]:\n",
        "\t\t\t\t\tposDocId = corpusDicoDocName[docId]\n",
        "\t\t\t\t\ttrueDocs[posDocId] = dicReqDocToKeep[req][docId]\n",
        "\n",
        "\t\tndcgBM25Cumul = ndcgBM25Cumul + ndcg_score([trueDocs], [doc_scores],k=ndcgTop)\n",
        "\t\tnbReq = nbReq + 1\n",
        "\n",
        "\tndcgBM25Cumul = ndcgBM25Cumul / nbReq\n",
        "\n",
        "\tprint(\"ndcg bm25=\",ndcgBM25Cumul)\n",
        "\treturn ndcgBM25Cumul\n",
        "\n",
        "nb_docs = 3192 #all docs\n",
        "#nb_docs = 150 #for tests\n",
        "run_bm25_only(0,nb_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HgRwMOaoRpW"
      },
      "source": [
        "It is now time to try and improve that model.\n",
        "I will start with a lemmatization and stemming in order to preprocess the corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8C3wEQLMoh3q",
        "outputId": "6fe27993-9712-4cc4-9eb3-d970c3838732"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Downloading additional NLTK data required for lemmatization\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Function to get wordnet part of speech from a simple part of speech tag\n",
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "# Lemmatization function\n",
        "def lemmatize_text(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(text)]\n",
        "    return lemmatized_tokens\n",
        "\n",
        "# Stemming function\n",
        "def stem_text(text):\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = [stemmer.stem(w) for w in nltk.word_tokenize(text)]\n",
        "    return stemmed_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN1HVlitpTZa"
      },
      "source": [
        "Now let's add these functions to the text2TokenList function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6F-TldJEoo6w"
      },
      "outputs": [],
      "source": [
        "def myText2TokenList(text, method=\"none\"):\n",
        "  stopword = stopwords.words('english')\n",
        "  word_tokens = word_tokenize(text.lower())\n",
        "  word_tokens_without_stops = [word for word in word_tokens if word not in stopword and len(word)>2]\n",
        "  # Applying the specified method\n",
        "  if method == \"lemmatize\":\n",
        "      return [lemmatize_text(word)[0] for word in word_tokens_without_stops]\n",
        "  elif method == \"stem\":\n",
        "      return [stem_text(word)[0] for word in word_tokens_without_stops]\n",
        "  else:\n",
        "      return word_tokens_without_stops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kI453nkIqvUv"
      },
      "source": [
        "We also can add synonyms to enhance the preprocessing. Let's keep in mind that depending on the context it can be less suitable to use them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zXV-Y7v8q2oB"
      },
      "outputs": [],
      "source": [
        "# Function to add synonyms\n",
        "def add_synonyms(tokens, max_synonyms=3):\n",
        "    enriched_tokens = []\n",
        "    for token in tokens:\n",
        "        # Add the original token\n",
        "        enriched_tokens.append(token)\n",
        "\n",
        "        # Retrieve synonyms from WordNet\n",
        "        synonyms = set()\n",
        "        for syn in wordnet.synsets(token):\n",
        "            for lemma in syn.lemmas()[:max_synonyms]:\n",
        "                synonym = lemma.name().replace('_', ' ').replace('-', ' ').lower()\n",
        "                synonyms.add(synonym)\n",
        "\n",
        "        # Add a limited number of synonyms to avoid excessive noise\n",
        "        enriched_tokens.extend(list(synonyms)[:max_synonyms])\n",
        "\n",
        "    return enriched_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlAtZqGEra8D"
      },
      "source": [
        "Let's now try the provided model with a better preprocesing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TnNEVUFJrKqE"
      },
      "outputs": [],
      "source": [
        "def updatedBm25(startDoc, endDoc, method=text2TokenList, model=BM25Okapi):\n",
        "\n",
        "\tdicDoc, dicReq, dicReqDoc = loadNFCorpus()\n",
        "\n",
        "\tdocsToKeep=[]\n",
        "\treqsToKeep=[]\n",
        "\tdicReqDocToKeep=defaultdict(dict)\n",
        "\n",
        "\tndcgTop=10\n",
        "\n",
        "\ti=startDoc\n",
        "\tfor reqId in dicReqDoc:\n",
        "\t\tif i > (endDoc - startDoc) :  #nbDocsToKeep:\n",
        "\t\t\tbreak\n",
        "\t\tfor docId in dicReqDoc[reqId]:\n",
        "\t\t\tdicReqDocToKeep[reqId][docId] = dicReqDoc[reqId][docId]\n",
        "\t\t\tdocsToKeep.append(docId)\n",
        "\t\t\ti = i + 1\n",
        "\t\treqsToKeep.append(reqId)\n",
        "\tdocsToKeep = list(set(docsToKeep))\n",
        "\n",
        "\t# Creates list of voc for docs and reqs:\n",
        "\tallVocab ={}\n",
        "\tfor k in docsToKeep:\n",
        "\t\tdocTokenList = method(dicDoc[k])\n",
        "\t\tfor word in docTokenList:\n",
        "\t\t\tif word not in allVocab:\n",
        "\t\t\t\tallVocab[word] = word\n",
        "\tallVocabListDoc = list(allVocab)\n",
        "\n",
        "\tallVocab ={}\n",
        "\tfor k in reqsToKeep:\n",
        "\t\tdocTokenList = method(dicReq[k])\n",
        "\t\tfor word in docTokenList:\n",
        "\t\t\tif word not in allVocab:\n",
        "\t\t\t\tallVocab[word] = word\n",
        "\tallVocabListReq = list(allVocab)\n",
        "\n",
        "\tcorpusDocTokenList = []\n",
        "\tcorpusReqTokenList = {}\n",
        "\n",
        "# Creates token lists for docs and reqs as well as dict of docs and reqs:\n",
        "  # corpusDocName == docsToKeep and corpusReqName == reqsToKeep\n",
        "\tcorpusDocName=[]\n",
        "\tcorpusDicoDocName={}\n",
        "\ti = 0\n",
        "\tfor k in docsToKeep:\n",
        "\t\tdocTokenList = method(dicDoc[k])\n",
        "\t\tcorpusDocTokenList.append(docTokenList)\n",
        "\t\tcorpusDocName.append(k)\n",
        "\t\tcorpusDicoDocName[k] = i\n",
        "\t\ti = i + 1\n",
        "\n",
        "\tcorpusReqName=[]\n",
        "\tcorpusDicoReqName={}\n",
        "\ti = 0\n",
        "\tfor k in reqsToKeep:\n",
        "\t\treqTokenList = method(dicReq[k])\n",
        "\t\tcorpusReqTokenList[k] = reqTokenList\n",
        "\t\tcorpusReqName.append(k)\n",
        "\t\tcorpusDicoReqName[k] = i\n",
        "\t\ti = i + 1\n",
        "\n",
        "\tchosenModel = model(corpusDocTokenList)\n",
        "\n",
        "\tndcgCumul=0\n",
        "\tcorpusReqVec={}\n",
        "\tndcgBM25Cumul=0\n",
        "\tnbReq=0\n",
        "\n",
        "\tfor req in corpusReqTokenList:\n",
        "\t\tj=0\n",
        "\t\treqTokenList = corpusReqTokenList[req]\n",
        "\t\tdoc_scores = chosenModel.get_scores(reqTokenList)\n",
        "\t\ttrueDocs = np.zeros(len(corpusDocTokenList))\n",
        "\n",
        "\t\tfor docId in corpusDicoDocName:\n",
        "\t\t\tif req in dicReqDocToKeep:\n",
        "\t\t\t\tif docId in dicReqDocToKeep[req]:\n",
        "\t\t\t\t\tposDocId = corpusDicoDocName[docId]\n",
        "\t\t\t\t\ttrueDocs[posDocId] = dicReqDocToKeep[req][docId]\n",
        "\n",
        "\t\tndcgBM25Cumul = ndcgBM25Cumul + ndcg_score([trueDocs], [doc_scores],k=ndcgTop)\n",
        "\t\tnbReq = nbReq + 1\n",
        "\n",
        "\tndcgBM25Cumul = ndcgBM25Cumul / nbReq\n",
        "\n",
        "\t#print(\"ndcg bm25=\",ndcgBM25Cumul)\n",
        "\treturn ndcgBM25Cumul"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpNp8qCKsFQq",
        "outputId": "b50974df-d74e-4641-f163-a5f264466047"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Provided:  0.43555347139300205\n",
            "Lematized:  0.4438042665332428\n",
            "Stemmed:  0.4480795606587508\n"
          ]
        }
      ],
      "source": [
        "def tokenListLematized(text):\n",
        "  return myText2TokenList(text, method=\"lemmatize\")\n",
        "\n",
        "def tokenListStemmed(text):\n",
        "  return myText2TokenList(text, method=\"stem\")\n",
        "\n",
        "print(\"Provided: \", updatedBm25(0,nb_docs))\n",
        "print(\"Lematized: \",updatedBm25(0,nb_docs, tokenListLematized))\n",
        "print(\"Stemmed: \", updatedBm25(0,nb_docs, tokenListStemmed))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HY5xcx3VxMR-",
        "outputId": "442ffd5a-1d6c-4213-f996-e8242dcdbb71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Synonyms:  0.41618666163242013\n",
            "SynonymsLem:  0.4086184450478863\n",
            "SynonymsStem:  0.3820931500611803\n"
          ]
        }
      ],
      "source": [
        "def add_synonymsProvided(text):\n",
        "  return add_synonyms(text2TokenList(text))\n",
        "\n",
        "def add_synonymsLem(text):\n",
        "  return add_synonyms(tokenListLematized(text))\n",
        "\n",
        "def add_synonymsStem(text):\n",
        "  return add_synonyms(tokenListStemmed(text))\n",
        "\n",
        "print(\"Synonyms: \",updatedBm25(0,nb_docs, add_synonymsProvided))\n",
        "print(\"SynonymsLem: \",updatedBm25(0,nb_docs, add_synonymsLem))\n",
        "print(\"SynonymsStem: \",updatedBm25(0,nb_docs, add_synonymsStem))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RlUaqA1yl7g"
      },
      "source": [
        "We can see that the stemming method alone is the one that gives the better ngcd, we'll then keep this one in preprocessing.\n",
        "\n",
        "To make easier the work we will separate the run_bm25_only function into three of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "isKXOauiHLVQ"
      },
      "outputs": [],
      "source": [
        "def load_and_tokenize(startDoc,endDoc):\n",
        "  dicDoc, dicReq, dicReqDoc = loadNFCorpus()\n",
        "\n",
        "  docsToKeep=[]\n",
        "  reqsToKeep=[]\n",
        "  dicReqDocToKeep=defaultdict(dict)\n",
        "\n",
        "  i=startDoc\n",
        "  for reqId in dicReqDoc:\n",
        "    if i > (endDoc - startDoc) :  #nbDocsToKeep:\n",
        "      break\n",
        "    for docId in dicReqDoc[reqId]:\n",
        "      dicReqDocToKeep[reqId][docId] = dicReqDoc[reqId][docId]\n",
        "      docsToKeep.append(docId)\n",
        "      i = i + 1\n",
        "    reqsToKeep.append(reqId)\n",
        "  docsToKeep = list(set(docsToKeep))\n",
        "\n",
        "  # Creates list of voc for docs and reqs:\n",
        "  allVocab ={}\n",
        "  for k in docsToKeep:\n",
        "    docTokenList = tokenListStemmed(dicDoc[k])\n",
        "    for word in docTokenList:\n",
        "      if word not in allVocab:\n",
        "        allVocab[word] = word\n",
        "  allVocabListDoc = list(allVocab)\n",
        "\n",
        "  allVocab ={}\n",
        "  for k in reqsToKeep:\n",
        "    docTokenList = tokenListStemmed(dicReq[k])\n",
        "    for word in docTokenList:\n",
        "      if word not in allVocab:\n",
        "        allVocab[word] = word\n",
        "  allVocabListReq = list(allVocab)\n",
        "\n",
        "  corpusDocTokenList = []\n",
        "  corpusDocTokenDict = {}\n",
        "  corpusReqTokenDict = {}\n",
        "\n",
        "  # Creates token lists for docs and reqs as well as dict of docs and reqs:\n",
        "    # corpusDocName == docsToKeep and corpusReqName == reqsToKeep\n",
        "  corpusDocName=[]\n",
        "  corpusDicoDocName={}\n",
        "  i = 0\n",
        "  for k in docsToKeep:\n",
        "    docTokenList = tokenListStemmed(dicDoc[k])\n",
        "    corpusDocTokenDict[k] = docTokenList\n",
        "    corpusDocTokenList.append(docTokenList)\n",
        "    corpusDocName.append(k)\n",
        "    corpusDicoDocName[k] = i\n",
        "    i = i + 1\n",
        "\n",
        "  corpusReqName=[]\n",
        "  corpusDicoReqName={}\n",
        "  i = 0\n",
        "  for k in reqsToKeep:\n",
        "    reqTokenList = tokenListStemmed(dicReq[k])\n",
        "    corpusReqTokenDict[k] = reqTokenList\n",
        "    corpusReqName.append(k)\n",
        "    corpusDicoReqName[k] = i\n",
        "    i = i + 1\n",
        "\n",
        "  return dicDoc, dicReq, dicReqDoc, corpusDocTokenList, corpusDocTokenDict, corpusReqTokenDict, corpusDicoDocName, dicReqDocToKeep\n",
        "\n",
        "def apply_bm25_model(corpusDocTokenList, corpusReqTokenDict, corpusDicoDocName, dicReqDocToKeep):\n",
        "  bm25 = BM25Okapi(corpusDocTokenList)\n",
        "  ndcgBM25Cumul=0\n",
        "  nbReq=0\n",
        "  ndcgTop=10\n",
        "  doc_scores_dict = {}\n",
        "  doc_scores_dict_named = {}\n",
        "\n",
        "  for req in corpusReqTokenDict:\n",
        "    j=0\n",
        "    reqTokenList = corpusReqTokenDict[req]\n",
        "    doc_scores = bm25.get_scores(reqTokenList)\n",
        "    doc_scores_dict[req] = doc_scores\n",
        "    trueDocs = np.zeros(len(corpusDocTokenList))\n",
        "\n",
        "    for docId in corpusDicoDocName:\n",
        "      if req in dicReqDocToKeep:\n",
        "        if docId in dicReqDocToKeep[req]:\n",
        "          posDocId = corpusDicoDocName[docId]\n",
        "          trueDocs[posDocId] = dicReqDocToKeep[req][docId]\n",
        "\n",
        "    ndcgBM25Cumul = ndcgBM25Cumul + ndcg_score([trueDocs], [doc_scores],k=ndcgTop)\n",
        "    nbReq = nbReq + 1\n",
        "\n",
        "  ndcgBM25Cumul = ndcgBM25Cumul / nbReq\n",
        "  print(\"ndcg bm25=\",ndcgBM25Cumul)\n",
        "  return ndcgBM25Cumul, doc_scores_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MgpZBXEprcE",
        "outputId": "33f8f815-c58f-49a7-eb4c-b83e737e5831"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ndcg bm25= 0.4480795606587508\n"
          ]
        }
      ],
      "source": [
        "dicDoc, dicReq, dicReqDoc, corpusDocTokenList, corpusDocTokenDict, corpusReqTokenDict, corpusDicoDocName, dicReqDocToKeep = load_and_tokenize(0,nb_docs)\n",
        "ndcgBM25Cumul, doc_scores_dict = apply_bm25_model(corpusDocTokenList, corpusReqTokenDict, corpusDicoDocName, dicReqDocToKeep)\n",
        "#print(doc_scores_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUhEercqzkAl"
      },
      "source": [
        "Let's try to use Word2Vec."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7h3OexHSxxb3"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Train Word2Vec model on the preprocessed corpus\n",
        "word2vec_model = Word2Vec(corpusDocTokenList, vector_size=300, window=3, min_count=1, workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Aye8zsaQkzWt"
      },
      "outputs": [],
      "source": [
        "# Function to vectorize a document using the Word2Vec model\n",
        "def vectorize_with_word2vec(text, model):\n",
        "    vectorized = [model.wv[word] for word in text if word in model.wv]\n",
        "    return np.mean(vectorized, axis=0) if vectorized else np.zeros(model.vector_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "umYv95QZVR7h"
      },
      "outputs": [],
      "source": [
        "# Vectorize documents\n",
        "doc_vectors = {doc_id: vectorize_with_word2vec(doc_tokens, word2vec_model) for doc_id, doc_tokens in corpusDocTokenDict.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "lQwiQYEGVSJF"
      },
      "outputs": [],
      "source": [
        "# Vectorize queries\n",
        "query_vectors = {query_id: vectorize_with_word2vec(query_tokens, word2vec_model) for query_id, query_tokens in corpusReqTokenDict.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gpgxPkI5luQC"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Function to retrieve and rank documents for a given query\n",
        "def retrieve_and_rank(query_vec, doc_vectors):\n",
        "    scores = {doc_id: cosine_similarity(query_vec.reshape(1, -1), doc_vec.reshape(1, -1))[0][0]\n",
        "              for doc_id, doc_vec in doc_vectors.items()}\n",
        "    ranked_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    return scores, ranked_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "bp98U7swwHlI"
      },
      "outputs": [],
      "source": [
        "ranked_results = {}\n",
        "scores = {}\n",
        "for query_id, query_vec in query_vectors.items():\n",
        "    scores[query_id], ranked_results[query_id] = retrieve_and_rank(query_vec, doc_vectors)\n",
        "\n",
        "#print('ranked_results', ranked_results['PLAIN-1'])\n",
        "#print('scores', scores['PLAIN-1'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "nBX6vjz7ZxKh"
      },
      "outputs": [],
      "source": [
        "def combine_scores(bm25_scores, ranked_results, alpha=0.5):\n",
        "    combined_scores = {}\n",
        "\n",
        "    for query_id in bm25_scores:\n",
        "        # Get BM25 scores for the current query\n",
        "        bm25_score_list = bm25_scores[query_id]\n",
        "\n",
        "        # Get Word2Vec scores for the current query\n",
        "        w2v_score_dict = ranked_results.get(query_id, {})\n",
        "\n",
        "        # Combine the scores\n",
        "        combined_query_scores = {}\n",
        "        for doc_idx, bm25_score in enumerate(bm25_score_list):\n",
        "            # Assuming document IDs in bm25_scores are in the same order as in w2v_score_dict\n",
        "            doc_id = list(w2v_score_dict.keys())[doc_idx] if doc_idx < len(w2v_score_dict) else None\n",
        "            if doc_id:\n",
        "                w2v_score = w2v_score_dict.get(doc_id, 0)\n",
        "                combined_score = alpha * bm25_score + (1 - alpha) * w2v_score\n",
        "                combined_query_scores[doc_id] = combined_score\n",
        "\n",
        "        # Store the combined scores for the query\n",
        "        combined_scores[query_id] = combined_query_scores\n",
        "\n",
        "    return combined_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "a7UMbji0BpiH"
      },
      "outputs": [],
      "source": [
        "combined_scores = combine_scores(doc_scores_dict, scores, alpha=0.5)\n",
        "#print(combined_scores[\"PLAIN-1\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ulc31S2uw5vw",
        "outputId": "17ac0f40-f715-4143-c550-593e04ef86e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The dictionary has been exported to /combined_scores.json\n"
          ]
        }
      ],
      "source": [
        "# Code to save the combined scores to be able to use the ranking in an App.\n",
        "import json\n",
        "\n",
        "file_path = '/combined_scores.json'\n",
        "\n",
        "with open(file_path, 'w') as file:\n",
        "    json.dump(combined_scores, file)\n",
        "print(f\"The dictionary has been exported to {file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-2EeRRCXcBr8"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import ndcg_score\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_with_ndcg(combined_scores, relevance_judgments):\n",
        "    ndcg_values = []\n",
        "\n",
        "    for query_id in combined_scores:\n",
        "        # Get the combined scores and the relevance judgments for the current query\n",
        "        query_scores = combined_scores[query_id]\n",
        "        query_judgments = relevance_judgments[query_id]\n",
        "\n",
        "        # Sort the documents by their combined scores\n",
        "        sorted_docs = sorted(query_scores, key=query_scores.get, reverse=True)\n",
        "\n",
        "        # Create lists of true relevances and predicted scores in the sorted order\n",
        "        true_relevances = [query_judgments.get(doc_id, 0) for doc_id in sorted_docs]\n",
        "        predicted_scores = [query_scores[doc_id] for doc_id in sorted_docs]\n",
        "\n",
        "        # Calculate NDCG for the current query\n",
        "        ndcg_value = ndcg_score([true_relevances], [predicted_scores])\n",
        "        ndcg_values.append(ndcg_value)\n",
        "\n",
        "    # Calculate the average NDCG across all queries\n",
        "    average_ndcg = np.mean(ndcg_values) if ndcg_values else 0\n",
        "    return average_ndcg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFhMj1c4HW04",
        "outputId": "795a2511-3ec6-490a-8013-a840cb201b64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.5912885589752738\n"
          ]
        }
      ],
      "source": [
        "mean_ndcg = evaluate_with_ndcg(combined_scores, dicReqDoc)\n",
        "print(mean_ndcg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JF2iQrfcgZL"
      },
      "source": [
        "We had 0.44 and now we have 0.5912885589752738 as ndgc.\n",
        "With for word2vec vector_size = 100, window = 3, min_count = 1"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
